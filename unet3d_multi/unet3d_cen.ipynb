{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:15:25.865086Z",
     "start_time": "2025-07-12T14:15:14.254718Z"
    }
   },
   "source": [
    "import os, json, time, csv, numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets.ADNI import ADNI, ADNI_transform\n",
    "from monai.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, matthews_corrcoef,\n",
    "                             confusion_matrix, roc_curve, auc)\n",
    "from models.unet3d import UNet3DClassifier, UNet3D, DualStreamUNet3DClassifier\n",
    "from utils.metrics import calculate_metrics\n",
    "from torch.multiprocessing import freeze_support"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:15:25.896021Z",
     "start_time": "2025-07-12T14:15:25.873094Z"
    }
   },
   "source": [
    "# -------------------- 配置 --------------------\n",
    "def load_cfg(path):\n",
    "    with open(path) as f: \n",
    "        return json.load(f)\n",
    "\n",
    "class Cfg:\n",
    "    def __init__(self, d):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        for k, v in d.items(): \n",
    "            setattr(self, k, v)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:15:26.219427Z",
     "start_time": "2025-07-12T14:15:26.207483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------- 创建模型 -------------------\n",
    "from models.unet3d import PartialCENUNet3DClassifier   # ← ① 新增：导入你刚实现的 CEN 模型\n",
    "\n",
    "def generate_model(cfg):\n",
    "    model = PartialCENUNet3DClassifier(                             # ← ② 使用 CEN 模型\n",
    "        in_ch_modality        = 1,                                  # MRI / PET 每模态 1 通道\n",
    "        num_classes           = cfg.nb_class,                       # 例如 2 (AD vs CN)\n",
    "        level_channels        = [64, 128, 256],\n",
    "        bottleneck_ch         = 512,\n",
    "        share_layers   = 2,               # 共享前 2 层\n",
    "        cen_ratios     = (0.20, 0.10)     # 对应每层的交换比例\n",
    "    ).to(cfg.device)\n",
    "\n",
    "    # 参数统计\n",
    "    total_params     = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    bytes_per_param  = 2 if getattr(cfg, 'fp16', False) else 4\n",
    "    print(\"--------------------model------------------\")\n",
    "    print(f\"Total params(M)    : {total_params:,}\")\n",
    "    print(f\"Trainable params(M): {trainable_params:,}\")\n",
    "    print(f\"Approx. size       : {total_params*bytes_per_param/1024**2:.2f} MB\")\n",
    "    print(\"model type:\", type(model).__name__)\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:15:26.297337Z",
     "start_time": "2025-07-12T14:15:26.239642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------- 加载配置 -------------------\n",
    "config_path = \"config/config.json\"\n",
    "cfg = Cfg(load_cfg(config_path))\n",
    "for name, val in vars(cfg).items():\n",
    "    print(f\"{name:15s}: {val}\")\n",
    "writer = SummaryWriter(cfg.checkpoint_dir)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device         : cuda:1\n",
      "label_file     : adni_dataset/ADNI_902.csv\n",
      "mri_dir        : adni_dataset/MRI\n",
      "pet_dir        : adni_dataset/PET\n",
      "task           : SMCIPMCI\n",
      "augment        : False\n",
      "split_ratio_test: 0.2\n",
      "seed           : 42\n",
      "num_epochs     : 100\n",
      "batch_size     : 8\n",
      "lr             : 1e-06\n",
      "weight_decay   : 1e-05\n",
      "fp16           : True\n",
      "checkpoint_dir : checkpoints_cen\n",
      "nb_class       : 2\n",
      "n_splits       : 5\n",
      "dropout_rate   : 0.5\n",
      "in_channels    : 2\n",
      "seg_task       : False\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:15:26.466004Z",
     "start_time": "2025-07-12T14:15:26.316244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------- 划分数据 -------------------\n",
    "fold_loaders = []                  # ⬅️ 所有折的 DataLoader 都收集到这里\n",
    "fold_indices = defaultdict(dict)   # 可选：若想保存索引，方便调试\n",
    "\n",
    "full_ds = ADNI(cfg.label_file, cfg.mri_dir, cfg.pet_dir,cfg.task, cfg.augment).data_dict\n",
    "labels  = [d['label'] for d in full_ds]\n",
    "\n",
    "outer_cv = StratifiedKFold(\n",
    "    n_splits=cfg.n_splits,     # 5 折\n",
    "    shuffle=True,\n",
    "    random_state=cfg.seed\n",
    ")\n",
    "\n",
    "for fold, (train_val_idx, test_idx) in enumerate(outer_cv.split(full_ds, labels), start=1):\n",
    "    train_val_ds = [full_ds[i] for i in train_val_idx]\n",
    "    test_ds      = [full_ds[i] for i in test_idx]\n",
    "\n",
    "    # —— 内层 90/10 分出验证集 —— #\n",
    "    labels_train_val = [d['label'] for d in train_val_ds]\n",
    "    idxs = np.arange(len(train_val_ds))\n",
    "    train_idx_, val_idx_ = train_test_split(\n",
    "        idxs, test_size=0.125, stratify=labels_train_val, random_state=cfg.seed\n",
    "    )\n",
    "    train_ds = [train_val_ds[i] for i in train_idx_]\n",
    "    val_ds   = [train_val_ds[i] for i in val_idx_]\n",
    "\n",
    "    print(f\"\\n=== Fold {fold}/{cfg.n_splits} ===\")\n",
    "    print(f\"训练集样本数: {len(train_ds)}  ({len(train_ds)/len(full_ds):.1%})\")\n",
    "    print(f\"验证集样本数: {len(val_ds)}  ({len(val_ds)/len(full_ds):.1%})\")\n",
    "    print(f\"测试集样本数: {len(test_ds)}  ({len(test_ds)/len(full_ds):.1%})\")\n",
    "    # —— 构造 DataLoader —— #\n",
    "    tr_tf, vl_tf = ADNI_transform(augment=cfg.augment)\n",
    "    te_tf        = vl_tf      # 测试不做增强\n",
    "    \n",
    "    tr_loader = DataLoader(\n",
    "        Dataset(train_ds, tr_tf),\n",
    "        batch_size=cfg.batch_size, shuffle=True,\n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    vl_loader = DataLoader(\n",
    "        Dataset(val_ds, vl_tf),\n",
    "        batch_size=cfg.batch_size, shuffle=False,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        Dataset(test_ds, te_tf),\n",
    "        batch_size=cfg.batch_size, shuffle=False,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # —— 保存到列表 —— #\n",
    "    fold_loaders.append({\n",
    "        \"fold\"        : fold,\n",
    "        \"train_loader\": tr_loader,\n",
    "        \"val_loader\"  : vl_loader,\n",
    "        \"test_loader\" : test_loader\n",
    "    })\n",
    "    \n",
    "    # （可选）保存索引，便于日后溯源\n",
    "    fold_indices[fold][\"train_idx\"] = train_idx_\n",
    "    fold_indices[fold][\"val_idx\"]   = val_idx_\n",
    "    fold_indices[fold][\"test_idx\"]  = test_idx\n",
    "    \n",
    "    # 现在 fold_loaders[0] ~ fold_loaders[4] 就是 5 组 train/val/test DataLoader\n",
    "    \n",
    "    save_path = os.path.join(cfg.checkpoint_dir, \"fold_indices.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        serializable = {\n",
    "            str(fold): {\n",
    "                \"train_idx\": v[\"train_idx\"].tolist(),\n",
    "                \"val_idx\"  : v[\"val_idx\"].tolist(),\n",
    "                \"test_idx\" : v[\"test_idx\"].tolist(),\n",
    "            }\n",
    "            for fold, v in fold_indices.items()\n",
    "        }\n",
    "        json.dump(serializable, f, indent=2)\n",
    "    print(f\"fold indices saved to {save_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ADNI Dataset: SMCIPMCI] 样本分布：\n",
      "  SMCI (0): 321\n",
      "  PMCI (1): 158\n",
      "\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "训练集样本数: 335  (69.9%)\n",
      "验证集样本数: 48  (10.0%)\n",
      "测试集样本数: 96  (20.0%)\n",
      "fold indices saved to checkpoints_cen\\fold_indices.json\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "训练集样本数: 335  (69.9%)\n",
      "验证集样本数: 48  (10.0%)\n",
      "测试集样本数: 96  (20.0%)\n",
      "fold indices saved to checkpoints_cen\\fold_indices.json\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "训练集样本数: 335  (69.9%)\n",
      "验证集样本数: 48  (10.0%)\n",
      "测试集样本数: 96  (20.0%)\n",
      "fold indices saved to checkpoints_cen\\fold_indices.json\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "训练集样本数: 335  (69.9%)\n",
      "验证集样本数: 48  (10.0%)\n",
      "测试集样本数: 96  (20.0%)\n",
      "fold indices saved to checkpoints_cen\\fold_indices.json\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "训练集样本数: 336  (70.1%)\n",
      "验证集样本数: 48  (10.0%)\n",
      "测试集样本数: 95  (19.8%)\n",
      "fold indices saved to checkpoints_cen\\fold_indices.json\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-12T14:15:26.516103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------- 五折交叉验证训练 -----------------\n",
    "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for fold_idx in range(cfg.n_splits):              # cfg.n_splits == 5\n",
    "    fold = fold_idx + 1\n",
    "    print(f\"\\n=== Fold {fold}/{cfg.n_splits} ===\")\n",
    "\n",
    "    # —— 每折都重新实例化模型与训练组件 —— #\n",
    "    model     = generate_model(cfg)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg.lr,\n",
    "        weight_decay=getattr(cfg, 'weight_decay', 0)\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.num_epochs)\n",
    "    scaler    = GradScaler(enabled=getattr(cfg, 'fp16', False))\n",
    "\n",
    "    # —— 获取该折的 DataLoader —— #\n",
    "    tr_loader = fold_loaders[fold_idx]['train_loader']\n",
    "    vl_loader = fold_loaders[fold_idx]['val_loader']\n",
    "\n",
    "    # —— 换成标准交叉熵 —— #\n",
    "    criterion = nn.CrossEntropyLoss()   # ⭐ 不再使用加权交叉熵 ⭐\n",
    "\n",
    "    # —— 为该折创建专属 CSV —— #\n",
    "    csv_path = os.path.join(cfg.checkpoint_dir, f\"metrics_fold{fold}.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"epoch\",\n",
    "            \"train_Loss\",\"train_ACC\",\"train_PRE\",\"train_SEN\",\"train_SPE\",\"train_F1\",\"train_AUC\",\"train_MCC\",\n",
    "            \"val_Loss\",\"val_ACC\"  ,\"val_PRE\"  ,\"val_SEN\"  ,\"val_SPE\"  ,\"val_F1\"  ,\"val_AUC\"  ,\"val_MCC\",\n",
    "        ])\n",
    "\n",
    "    best_auc = -np.inf\n",
    "\n",
    "    # —— Epoch 循环 —— #\n",
    "    for epoch in range(1, cfg.num_epochs + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # -------- Train --------\n",
    "        model.train()\n",
    "        tr_loss_sum = 0.0\n",
    "        tr_batches  = 0\n",
    "        yt, yp, ys = [], [], []\n",
    "        for batch in tr_loader:\n",
    "            mri = batch['MRI'].to(cfg.device)      # [B,1,D,H,W]\n",
    "            pet = batch['PET'].to(cfg.device)      # [B,1,D,H,W]\n",
    "            y   = batch['label'].to(cfg.device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(device_type='cuda', enabled=getattr(cfg, 'fp16', False)):\n",
    "                out  = model(mri,pet)\n",
    "                loss = criterion(out, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            tr_loss_sum += loss.item()\n",
    "            tr_batches  += 1\n",
    "\n",
    "            prob = torch.softmax(out, dim=1)[:, 1].detach().cpu().numpy()\n",
    "            pred = out.argmax(1).detach().cpu().numpy()\n",
    "            yt.extend(y.cpu().numpy())\n",
    "            yp.extend(pred)\n",
    "            ys.extend(prob)\n",
    "\n",
    "        tr_met  = calculate_metrics(yt, yp, ys)\n",
    "        tr_loss = tr_loss_sum / tr_batches\n",
    "\n",
    "        # -------- Validation --------\n",
    "        model.eval()\n",
    "        vl_loss_sum = 0.0\n",
    "        vl_batches  = 0\n",
    "        yt, yp, ys = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in vl_loader:\n",
    "                mri = batch['MRI'].to(cfg.device)\n",
    "                pet = batch['PET'].to(cfg.device)\n",
    "                y   = batch['label'].to(cfg.device).long()\n",
    "\n",
    "                with autocast(device_type='cuda', enabled=getattr(cfg, 'fp16', False)):\n",
    "                    out  = model(mri,pet)\n",
    "                    loss = criterion(out, y)\n",
    "\n",
    "                vl_loss_sum += loss.item()\n",
    "                vl_batches  += 1\n",
    "\n",
    "                prob = torch.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "                pred = out.argmax(1).cpu().numpy()\n",
    "                yt.extend(y.cpu().numpy())\n",
    "                yp.extend(pred)\n",
    "                ys.extend(prob)\n",
    "\n",
    "        vl_met  = calculate_metrics(yt, yp, ys)\n",
    "        vl_loss = vl_loss_sum / vl_batches\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Fold {fold} | Epoch {epoch:03d} | \"\n",
    "            f\"Train Loss={tr_loss:.4f} | Val Loss={vl_loss:.4f} | \"\n",
    "            f\"Train ACC={tr_met['ACC']:.4f} | Val ACC={vl_met['ACC']:.4f} | \"\n",
    "            f\"Train AUC={tr_met['AUC']:.4f} | Val AUC={vl_met['AUC']:.4f} | \"\n",
    "            f\"time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        # —— 保存当前折最佳模型 —— #\n",
    "        if vl_met['AUC'] > best_auc:\n",
    "            best_auc = vl_met['AUC']\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(cfg.checkpoint_dir, f\"best_model_fold{fold}.pth\")\n",
    "            )\n",
    "            print(\"✅ Fold\", fold, \"saved best model (AUC={:.4f})\".format(best_auc))\n",
    "\n",
    "        # —— 追加写入 CSV —— #\n",
    "        with open(csv_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                epoch,\n",
    "                f\"{tr_loss:.4f}\", f\"{tr_met['ACC']:.4f}\", f\"{tr_met['PRE']:.4f}\",\n",
    "                f\"{tr_met['SEN']:.4f}\", f\"{tr_met['SPE']:.4f}\", f\"{tr_met['F1']:.4f}\", f\"{tr_met['AUC']:.4f}\", f\"{tr_met['MCC']:.4f}\",\n",
    "                f\"{vl_loss:.4f}\", f\"{vl_met['ACC']:.4f}\", f\"{vl_met['PRE']:.4f}\",\n",
    "                f\"{vl_met['SEN']:.4f}\", f\"{vl_met['SPE']:.4f}\", f\"{vl_met['F1']:.4f}\", f\"{vl_met['AUC']:.4f}\", f\"{vl_met['MCC']:.4f}\",\n",
    "            ])\n",
    "\n",
    "    print(f\"=== Fold {fold} 完成，Best AUC={best_auc:.4f} ===\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "--------------------model------------------\n",
      "Total params(M)    : 13,669,378\n",
      "Trainable params(M): 13,669,378\n",
      "Approx. size       : 26.07 MB\n",
      "model type: PartialCENUNet3DClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongzj\\AppData\\Local\\Temp\\ipykernel_30756\\1643430249.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler(enabled=getattr(cfg, 'fp16', False))\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:09:24.274674600Z",
     "start_time": "2025-07-12T13:59:34.338738Z"
    }
   },
   "source": [
    "# -----------------------测试-----------------------\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def load_test_data(cfg, fold):\n",
    "    full_ds = ADNI(\n",
    "        cfg.label_file,\n",
    "        cfg.mri_dir,\n",
    "        cfg.pet_dir,\n",
    "        cfg.task,\n",
    "        cfg.augment\n",
    "    ).data_dict\n",
    "\n",
    "    idx_path = os.path.join(cfg.checkpoint_dir, \"fold_indices.json\")\n",
    "    with open(idx_path, \"r\") as f:\n",
    "        all_indices = json.load(f)\n",
    "\n",
    "    test_idx = all_indices[str(fold)][\"test_idx\"]\n",
    "    test_data = [full_ds[i] for i in test_idx]\n",
    "    return test_data\n",
    "\n",
    "def test_models(checkpoint_dir, test_data, fold):\n",
    "    \"\"\"返回 metrics, y_prob, y_true, y_pred （新增 y_pred）\"\"\"\n",
    "    device = cfg.device\n",
    "\n",
    "    _, test_tf = ADNI_transform(augment=False)\n",
    "    ds = Dataset(data=test_data, transform=test_tf)\n",
    "    loader = DataLoader(ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                        num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = generate_model(cfg)\n",
    "    ckpt = os.path.join(checkpoint_dir, f\"best_model_fold{fold}.pth\")\n",
    "    \n",
    "    # ---- 安全加载 state_dict ----\n",
    "    try:\n",
    "        state_dict = torch.load(ckpt, map_location=device, weights_only=True)\n",
    "    except TypeError:  # 兼容旧版 PyTorch\n",
    "        state_dict = torch.load(ckpt, map_location=device)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device).eval()\n",
    "    print(f\"✅ Loaded {ckpt}\")\n",
    "\n",
    "    y_true, y_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            mri = batch['MRI'].to(cfg.device)\n",
    "            pet = batch['PET'].to(cfg.device)\n",
    "            y   = batch['label'].to(cfg.device).long()\n",
    "\n",
    "            out = model(mri,pet)\n",
    "            \n",
    "            probs = torch.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "            labels = batch['label'].long().view(-1).cpu().numpy()\n",
    "            y_prob.extend(probs)\n",
    "            y_true.extend(labels)\n",
    "\n",
    "    y_pred  = (np.array(y_prob) > 0.5).astype(int)\n",
    "    metrics = calculate_metrics(y_true, y_pred, y_prob)\n",
    "\n",
    "    # --- ROC ---\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fpr, tpr, lw=2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title(f'ROC Fold {fold} (AUC={metrics[\"AUC\"]:.2f})')\n",
    "    roc_path = os.path.join(checkpoint_dir, f\"roc_fold{fold}.png\")\n",
    "    plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"✅ ROC curve for fold {fold} saved to {roc_path}\")\n",
    "\n",
    "    return metrics, y_prob, y_true, y_pred   # <── 新增 y_pred\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:09:24.275670400Z",
     "start_time": "2025-07-12T13:59:40.623801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取配置\n",
    "config_path = \"config\\config.json\"\n",
    "cfg = Cfg(load_cfg(config_path))\n",
    "\n",
    "# 统计一次模型参数\n",
    "temp_model = generate_model(cfg)\n",
    "total_params     = sum(p.numel() for p in temp_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in temp_model.parameters() if p.requires_grad)\n",
    "bytes_per_param  = 2 if getattr(cfg, 'fp16', False) else 4\n",
    "approx_size_mb   = total_params * bytes_per_param / 1024 ** 2\n",
    "del temp_model\n",
    "\n",
    "#------------- 文件准备 -------------\n",
    "all_metrics = []\n",
    "all_probs   = []\n",
    "all_labels  = []\n",
    "\n",
    "ckpt_dir = cfg.checkpoint_dir\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "results_txt  = os.path.join(ckpt_dir, \"test_results.txt\")\n",
    "result_csv   = os.path.join(ckpt_dir, \"result.csv\")  # 新增\n",
    "\n",
    "# TXT：模型参数 + 表头\n",
    "with open(results_txt, \"w\") as f:\n",
    "    f.write(\"===== MODEL PARAMETERS =====\\n\")\n",
    "    f.write(f\"Total params       : {total_params}\\n\")\n",
    "    f.write(f\"Trainable params   : {trainable_params}\\n\")\n",
    "    f.write(f\"Approx. size (MB)  : {approx_size_mb:.2f}\\n\\n\")\n",
    "    f.write(\"===== FOLD RESULTS =====\\n\")\n",
    "    f.write(\"Fold\\tACC\\tPRE\\tSEN\\tSPE\\tF1\\tAUC\\tMCC\\n\")\n",
    "\n",
    "# CSV：表头\n",
    "with open(result_csv, \"w\", newline=\"\") as csv_f:\n",
    "    writer = csv.writer(csv_f)\n",
    "    writer.writerow([\n",
    "        \"fold\", \"idx_in_fold\", \"sample_id\",\n",
    "        \"true_label\", \"pred_label\", \"correct\"\n",
    "    ])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------model------------------\n",
      "Total params(M)    : 13,669,378\n",
      "Trainable params(M): 13,669,378\n",
      "Approx. size       : 26.07 MB\n",
      "model type: PartialCENUNet3DClassifier\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T14:09:24.275670400Z",
     "start_time": "2025-07-12T13:59:42.572654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#------------- 逐折测试 -------------\n",
    "for fold in range(1, cfg.n_splits + 1):\n",
    "    print(f\"\\n=== Testing Fold {fold}/{cfg.n_splits} ===\")\n",
    "    test_data = load_test_data(cfg, fold)\n",
    "\n",
    "    # metrics, probs, labels, preds\n",
    "    metrics, probs, labels, preds = test_models(\n",
    "        ckpt_dir, test_data, fold\n",
    "    )\n",
    "\n",
    "    # Console 输出\n",
    "    print(\n",
    "        f\"Fold {fold} - \"\n",
    "        f\"ACC={metrics['ACC']:.4f}, PRE={metrics['PRE']:.4f}, \"\n",
    "        f\"SEN={metrics['SEN']:.4f}, SPE={metrics['SPE']:.4f}, \"\n",
    "        f\"F1={metrics['F1']:.4f}, AUC={metrics['AUC']:.4f}, \"\n",
    "        f\"MCC={metrics['MCC']:.4f}\"\n",
    "    )\n",
    "\n",
    "    # TXT 写入\n",
    "    with open(results_txt, \"a\") as f:\n",
    "        f.write(\n",
    "            f\"{fold}\\t\"\n",
    "            f\"{metrics['ACC']:.4f}\\t{metrics['PRE']:.4f}\\t\"\n",
    "            f\"{metrics['SEN']:.4f}\\t{metrics['SPE']:.4f}\\t\"\n",
    "            f\"{metrics['F1']:.4f}\\t{metrics['AUC']:.4f}\\t\"\n",
    "            f\"{metrics['MCC']:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "    # CSV：样本级结果\n",
    "    with open(result_csv, \"a\", newline=\"\") as csv_f:\n",
    "        writer = csv.writer(csv_f)\n",
    "        for idx, (sample_dict, y_t, y_p) in enumerate(\n",
    "                zip(test_data, labels, preds)):\n",
    "            # 尝试从样本 dict 中抓 ID；若无则用文件名或序号\n",
    "            sample_id = (\n",
    "                sample_dict.get(\"subject\")\n",
    "                or os.path.basename(sample_dict.get(\"MRI\", f\"s{idx}\"))\n",
    "            )\n",
    "            writer.writerow([\n",
    "                fold, idx, sample_id,\n",
    "                int(y_t), int(y_p), int(y_t == y_p)\n",
    "            ])\n",
    "\n",
    "    # 汇总\n",
    "    all_metrics.append(metrics)\n",
    "    all_probs.extend(probs)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "#------------- 平均 ROC -------------\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "plt.plot(mean_fpr, interp_tpr, 'b-', lw=2,\n",
    "         label=f'Mean ROC (AUC={roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(os.path.join(ckpt_dir, 'mean_test_roc.png'),\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "#------------- 汇总指标 -------------\n",
    "print(\"\\n=== Final Test Results (mean ± std) ===\")\n",
    "summary_lines = []\n",
    "for k in ['ACC', 'PRE', 'SEN', 'SPE', 'F1', 'AUC', 'MCC']:\n",
    "    vals = [m[k] for m in all_metrics]\n",
    "    mean_val = np.mean(vals)\n",
    "    std_val  = np.std(vals)\n",
    "    line = f\"{k}: {mean_val:.4f} ± {std_val:.4f}\"\n",
    "    print(line)\n",
    "    summary_lines.append(line)\n",
    "\n",
    "with open(results_txt, \"a\") as f:\n",
    "    f.write(\"\\n===== SUMMARY =====\\n\")\n",
    "    for line in summary_lines:\n",
    "        f.write(line + \"\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Fold 1/5 ===\n",
      "\n",
      "[ADNI Dataset: SMCIPMCI] 样本分布：\n",
      "  SMCI (0): 321\n",
      "  PMCI (1): 158\n",
      "\n",
      "--------------------model------------------\n",
      "Total params(M)    : 13,669,378\n",
      "Trainable params(M): 13,669,378\n",
      "Approx. size       : 26.07 MB\n",
      "model type: PartialCENUNet3DClassifier\n",
      "✅ Loaded checkpoints_cen\\best_model_fold1.pth\n",
      "✅ ROC curve for fold 1 saved to checkpoints_cen\\roc_fold1.png\n",
      "Fold 1 - ACC=0.6979, PRE=0.5833, SEN=0.2258, SPE=0.9231, F1=0.3256, AUC=0.6060, MCC=0.2105\n",
      "\n",
      "=== Testing Fold 2/5 ===\n",
      "\n",
      "[ADNI Dataset: SMCIPMCI] 样本分布：\n",
      "  SMCI (0): 321\n",
      "  PMCI (1): 158\n",
      "\n",
      "--------------------model------------------\n",
      "Total params(M)    : 13,669,378\n",
      "Trainable params(M): 13,669,378\n",
      "Approx. size       : 26.07 MB\n",
      "model type: PartialCENUNet3DClassifier\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoints_cen\\\\best_model_fold2.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m test_data \u001B[38;5;241m=\u001B[39m load_test_data(cfg, fold)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# metrics, probs, labels, preds\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m metrics, probs, labels, preds \u001B[38;5;241m=\u001B[39m \u001B[43mtest_models\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mckpt_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfold\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Console 输出\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFold \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mACC=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mACC\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, PRE=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPRE\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMCC=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMCC\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     18\u001B[0m )\n",
      "Cell \u001B[1;32mIn[7], line 36\u001B[0m, in \u001B[0;36mtest_models\u001B[1;34m(checkpoint_dir, test_data, fold)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# ---- 安全加载 state_dict ----\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 36\u001B[0m     state_dict \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mckpt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:  \u001B[38;5;66;03m# 兼容旧版 PyTorch\u001B[39;00m\n\u001B[0;32m     38\u001B[0m     state_dict \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(ckpt, map_location\u001B[38;5;241m=\u001B[39mdevice)\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:1065\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m   1062\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m   1063\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m-> 1065\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m   1066\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m   1067\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m   1068\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m   1069\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m   1070\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:468\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 468\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    469\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    470\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:449\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    448\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 449\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'checkpoints_cen\\\\best_model_fold2.pth'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
