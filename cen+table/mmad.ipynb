{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-14T05:49:23.684421Z",
     "start_time": "2025-07-14T05:49:23.671398Z"
    }
   },
   "source": [
    "import os, json, time, csv, numpy as np, pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets.ADNI import ADNI, ADNI_transform\n",
    "from monai.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from utils.metrics import calculate_metrics\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:49:23.745737Z",
     "start_time": "2025-07-14T05:49:23.712767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------- 配置 --------------------\n",
    "def load_cfg(path):\n",
    "    with open(path) as f: \n",
    "        return json.load(f)\n",
    "\n",
    "class Cfg:\n",
    "    def __init__(self, d):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        for k, v in d.items(): \n",
    "            setattr(self, k, v)\n",
    "# ----------------- 加载配置 -------------------\n",
    "config_path = \"config/config.json\"\n",
    "cfg = Cfg(load_cfg(config_path))\n",
    "for name, val in vars(cfg).items():\n",
    "    print(f\"{name:15s}: {val}\")\n",
    "writer = SummaryWriter(cfg.checkpoint_dir)"
   ],
   "id": "d6109517638733bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device         : cuda:1\n",
      "label_file     : adni_dataset/ADNI_902.csv\n",
      "mri_dir        : adni_dataset/MRI\n",
      "pet_dir        : adni_dataset/PET\n",
      "table_dir      : adni_dataset/ADNI_Tabel.csv\n",
      "tabular_emb    : models/tabular_emb.csv\n",
      "table_startcol : 4\n",
      "task           : SMCIPMCI\n",
      "augment        : False\n",
      "split_ratio_test: 0.2\n",
      "seed           : 42\n",
      "num_epochs     : 100\n",
      "batch_size     : 6\n",
      "lr             : 1e-06\n",
      "weight_decay   : 1e-05\n",
      "fp16           : True\n",
      "checkpoint_dir : checkpoints_mmad-mci\n",
      "nb_class       : 2\n",
      "n_splits       : 5\n",
      "dropout_rate   : 0.5\n",
      "in_channels    : 2\n",
      "seg_task       : False\n",
      "img_dim        : 512\n",
      "tab_dim        : 192\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:49:23.913949Z",
     "start_time": "2025-07-14T05:49:23.857203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "full_dataset = ADNI(cfg.label_file, cfg.mri_dir, cfg.pet_dir, cfg.task, cfg.augment)\n",
    "full_ds      = full_dataset.data_dict         # list[dict]\n",
    "labels       = [d[\"label\"] for d in full_ds]\n",
    "\n",
    "# -------------------- 划分 --------------------\n",
    "\n",
    "fold_indices = defaultdict(dict)\n",
    "outer_cv = StratifiedKFold(\n",
    "    n_splits=cfg.n_splits, shuffle=True, random_state=cfg.seed\n",
    ")\n",
    "\n",
    "for fold, (train_val_idx, test_idx) in enumerate(outer_cv.split(full_ds, labels), start=1):\n",
    "    # ——— 内层 90/10 再分验证集 ———\n",
    "    train_val_labels = [labels[i] for i in train_val_idx]\n",
    "    idxs_inner       = np.arange(len(train_val_idx))\n",
    "    train_idx_in, val_idx_in = train_test_split(\n",
    "        idxs_inner, test_size=0.125, stratify=train_val_labels, random_state=cfg.seed\n",
    "    )\n",
    "\n",
    "    # ——— 映射回 full_ds 的绝对索引 ———\n",
    "    train_idx = np.array(train_val_idx)[train_idx_in]\n",
    "    val_idx   = np.array(train_val_idx)[val_idx_in]\n",
    "\n",
    "    fold_indices[fold][\"train_idx\"] = train_idx.tolist()\n",
    "    fold_indices[fold][\"val_idx\"]   = val_idx.tolist()\n",
    "    fold_indices[fold][\"test_idx\"]  = test_idx.tolist()\n",
    "\n",
    "    print(f\"Fold {fold}: train {len(train_idx)}, val {len(val_idx)}, test {len(test_idx)}\")\n",
    "\n",
    "# -------------------- 保存 JSON --------------------\n",
    "\n",
    "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
    "json_path = os.path.join(cfg.checkpoint_dir, \"fold_indices.json\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump({str(k): v for k, v in fold_indices.items()}, f, indent=2)\n",
    "print(f\"Fold indices saved to {json_path}\")"
   ],
   "id": "6328bbec6694dae2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ADNI Dataset: SMCIPMCI] 样本分布：\n",
      "  SMCI (0): 321\n",
      "  PMCI (1): 158\n",
      "\n",
      "Fold 1: train 335, val 48, test 96\n",
      "Fold 2: train 335, val 48, test 96\n",
      "Fold 3: train 335, val 48, test 96\n",
      "Fold 4: train 335, val 48, test 96\n",
      "Fold 5: train 336, val 48, test 95\n",
      "Fold indices saved to checkpoints_mmad-mci\\fold_indices.json\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:49:24.283114Z",
     "start_time": "2025-07-14T05:49:24.011680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 假设 full_ds = ADNI(...).data_dict，cfg 已定义\n",
    "json_path = os.path.join(cfg.checkpoint_dir, \"fold_indices.json\")\n",
    "csv_path  = os.path.join(cfg.table_dir)  # 请替换为你的实际文件名\n",
    "\n",
    "# 读取完整表格\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 读取 fold 索引\n",
    "with open(json_path, 'r') as f:\n",
    "    fold_indices = json.load(f)\n",
    "\n",
    "# 提取 full_ds 中的 Subject_ID 列表\n",
    "all_subjects = [entry['Subject'] for entry in full_ds]\n",
    "\n",
    "for fold_str, idxs in fold_indices.items():\n",
    "    fold = int(fold_str)\n",
    "    train_idx = idxs['train_idx']\n",
    "    val_idx   = idxs['val_idx']\n",
    "    test_idx  = idxs['test_idx']\n",
    "\n",
    "    # 根据索引得到本折的 Subject_ID 列表\n",
    "    train_subs = [ all_subjects[i] for i in train_idx ]\n",
    "    val_subs   = [ all_subjects[i] for i in val_idx   ]\n",
    "    test_subs  = [ all_subjects[i] for i in test_idx  ]\n",
    "\n",
    "    # 在原始 df 中筛出对应行\n",
    "    df_train = df[df['Subject_ID'].isin(train_subs)].reset_index(drop=True)\n",
    "    df_val   = df[df['Subject_ID'].isin(val_subs)]  .reset_index(drop=True)\n",
    "    df_test  = df[df['Subject_ID'].isin(test_subs)] .reset_index(drop=True)\n",
    "\n",
    "    # 重排列顺序：第一列 Subject_ID，第二列 Group，后面是所有其他列\n",
    "    def reorder(df_split):\n",
    "        cols = df_split.columns.tolist()\n",
    "        cols.remove('Subject_ID')\n",
    "        if 'Group' not in cols:\n",
    "            raise KeyError(\"'Group' 列未找到，请确认表格中有此列\")\n",
    "        cols.remove('Group')\n",
    "        return df_split[['Subject_ID', 'Group'] + cols]\n",
    "\n",
    "    df_train = reorder(df_train)\n",
    "    df_val   = reorder(df_val)\n",
    "    df_test  = reorder(df_test)\n",
    "\n",
    "    # 保存到各自目录\n",
    "    out_dir = os.path.join(cfg.checkpoint_dir, f\"fold{fold}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df_train.to_csv(os.path.join(out_dir, \"train.csv\"), index=False)\n",
    "    df_val  .to_csv(os.path.join(out_dir, \"val.csv\"),   index=False)\n",
    "    df_test .to_csv(os.path.join(out_dir, \"test.csv\"),  index=False)\n",
    "\n",
    "    print(f\"Fold {fold} saved:\")\n",
    "    print(f\"  train → {os.path.join(out_dir, 'train.csv')}\")\n",
    "    print(f\"  val   → {os.path.join(out_dir, 'val.csv')}\")\n",
    "    print(f\"  test  → {os.path.join(out_dir, 'test.csv')}\")\n"
   ],
   "id": "496d0f740e77f37c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 saved:\n",
      "  train → checkpoints_mmad-mci\\fold1\\train.csv\n",
      "  val   → checkpoints_mmad-mci\\fold1\\val.csv\n",
      "  test  → checkpoints_mmad-mci\\fold1\\test.csv\n",
      "Fold 2 saved:\n",
      "  train → checkpoints_mmad-mci\\fold2\\train.csv\n",
      "  val   → checkpoints_mmad-mci\\fold2\\val.csv\n",
      "  test  → checkpoints_mmad-mci\\fold2\\test.csv\n",
      "Fold 3 saved:\n",
      "  train → checkpoints_mmad-mci\\fold3\\train.csv\n",
      "  val   → checkpoints_mmad-mci\\fold3\\val.csv\n",
      "  test  → checkpoints_mmad-mci\\fold3\\test.csv\n",
      "Fold 4 saved:\n",
      "  train → checkpoints_mmad-mci\\fold4\\train.csv\n",
      "  val   → checkpoints_mmad-mci\\fold4\\val.csv\n",
      "  test  → checkpoints_mmad-mci\\fold4\\test.csv\n",
      "Fold 5 saved:\n",
      "  train → checkpoints_mmad-mci\\fold5\\train.csv\n",
      "  val   → checkpoints_mmad-mci\\fold5\\val.csv\n",
      "  test  → checkpoints_mmad-mci\\fold5\\test.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:50:31.319982Z",
     "start_time": "2025-07-14T05:49:24.378735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.tabular_encoder import tabular_encoder_fold\n",
    "\n",
    "for fold in range(1, cfg.n_splits+1):\n",
    "    fold_dir = os.path.join(cfg.checkpoint_dir, f\"fold{fold}\")\n",
    "    if cfg.task == \"ADCN\":\n",
    "        classes = [\"CN\", \"AD\"]\n",
    "    elif cfg.task == \"SMCIPMCI\":\n",
    "        classes = [\"SMCI\", \"PMCI\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task: {cfg.task}\")\n",
    "    tabular_encoder_fold(\n",
    "        fold_dir    = fold_dir,\n",
    "        label_col   = \"Group\",\n",
    "        classes     = classes,\n",
    "        start_col   = 3,\n",
    "        device      = cfg.device,\n",
    "        n_fold      = 5,\n",
    "        dropna      = False\n",
    "    )\n",
    "    \n",
    "\n"
   ],
   "id": "24322c3dbde3a27a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Using device: cuda:0\n",
      "✓ Saved train_emb.csv ((335, 194))\n",
      "✓ Saved val_emb.csv ((48, 194))\n",
      "✓ Saved test_emb.csv ((96, 194))\n",
      "✓ Saved train_emb.csv ((335, 194))\n",
      "✓ Saved val_emb.csv ((48, 194))\n",
      "✓ Saved test_emb.csv ((96, 194))\n",
      "✓ Saved train_emb.csv ((335, 194))\n",
      "✓ Saved val_emb.csv ((48, 194))\n",
      "✓ Saved test_emb.csv ((96, 194))\n",
      "✓ Saved train_emb.csv ((335, 194))\n",
      "✓ Saved val_emb.csv ((48, 194))\n",
      "✓ Saved test_emb.csv ((96, 194))\n",
      "✓ Saved train_emb.csv ((336, 194))\n",
      "✓ Saved val_emb.csv ((48, 194))\n",
      "✓ Saved test_emb.csv ((95, 194))\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:50:31.721108Z",
     "start_time": "2025-07-14T05:50:31.430494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------------------------------\n",
    "# 1. 建立 Subject ➜ 影像样本字典，便于快速对齐\n",
    "# -------------------------------------------------\n",
    "subject_map = {d[\"Subject\"]: d for d in full_ds}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. 为五折构造 train/val/test DataLoader\n",
    "# -------------------------------------------------\n",
    "import pandas as pd\n",
    "from monai.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_emb_csv(path):\n",
    "    \"\"\"CSV -> {sid: (label:int, emb:numpy[192])}\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    sid   = df.iloc[:, 0].astype(str).tolist()\n",
    "    label = df.iloc[:, 1].astype(int).tolist()\n",
    "    emb   = df.iloc[:, 2:].astype(\"float32\").values\n",
    "    return {s: (l, e) for s, l, e in zip(sid, label, emb)}\n",
    "\n",
    "tr_tf, vl_tf = ADNI_transform(augment=cfg.augment)\n",
    "te_tf        = vl_tf\n",
    "\n",
    "fold_loaders = []\n",
    "\n",
    "for fold in range(1, cfg.n_splits + 1):\n",
    "    fold_dir  = os.path.join(cfg.checkpoint_dir, f\"fold{fold}\")\n",
    "    paths     = {sp: os.path.join(fold_dir, f\"{sp}_emb.csv\")\n",
    "                 for sp in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "    # 解析 CSV\n",
    "    emb_maps  = {sp: load_emb_csv(p) for sp, p in paths.items()}\n",
    "\n",
    "    split_ds  = {}\n",
    "    for sp, emb_map in emb_maps.items():\n",
    "        samples = []\n",
    "        for sid, (lbl, emb) in emb_map.items():\n",
    "            if sid not in subject_map:\n",
    "                raise KeyError(f\"{sid} not found in ADNI dataset\")\n",
    "            s = subject_map[sid].copy()     # MRI / PET / label / Subject\n",
    "            s[\"label\"] = lbl                # 以 CSV 为准\n",
    "            s[\"table\"] = emb                # 192‑d numpy\n",
    "            samples.append(s)\n",
    "        split_ds[sp] = samples\n",
    "\n",
    "    # DataLoader\n",
    "    dl_kw = dict(batch_size=cfg.batch_size, pin_memory=True)\n",
    "    fold_loaders.append({\n",
    "        \"fold\": fold,\n",
    "        \"train_loader\": DataLoader(Dataset(split_ds[\"train\"], tr_tf),\n",
    "                                   shuffle=True,  num_workers=4, **dl_kw),\n",
    "        \"val_loader\":   DataLoader(Dataset(split_ds[\"val\"],   vl_tf),\n",
    "                                   shuffle=False, num_workers=2, **dl_kw),\n",
    "        \"test_loader\":  DataLoader(Dataset(split_ds[\"test\"],  te_tf),\n",
    "                                   shuffle=False, num_workers=2, **dl_kw),\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {fold}  ➜  train {len(split_ds['train'])} | \"\n",
    "          f\"val {len(split_ds['val'])} | test {len(split_ds['test'])}\")\n",
    "\n",
    "# 现在 fold_loaders 就和之前影像-only 版本一模一样可直接用于训练：\n",
    "# for fold_dict in fold_loaders:\n",
    "#     tr_loader = fold_dict[\"train_loader\"]\n",
    "#     ...\n"
   ],
   "id": "376d3ed5f64e73ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1  ➜  train 335 | val 48 | test 96\n",
      "Fold 2  ➜  train 335 | val 48 | test 96\n",
      "Fold 3  ➜  train 335 | val 48 | test 96\n",
      "Fold 4  ➜  train 335 | val 48 | test 96\n",
      "Fold 5  ➜  train 336 | val 48 | test 95\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:54:20.754308Z",
     "start_time": "2025-07-14T05:50:31.816687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inspect_fold_loaders(fold_loaders):\n",
    "    for fd in fold_loaders:\n",
    "        fold = fd[\"fold\"]\n",
    "        tl, vl, te = fd[\"train_loader\"], fd[\"val_loader\"], fd[\"test_loader\"]\n",
    "\n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        print(f\"  ▸ train_loader ─ {len(tl.dataset):4d} samples \"\n",
    "              f\"· {len(tl):3d} batches (batch={tl.batch_size})\")\n",
    "        print(f\"  ▸ val_loader   ─ {len(vl.dataset):4d} samples \"\n",
    "              f\"· {len(vl):3d} batches\")\n",
    "        print(f\"  ▸ test_loader  ─ {len(te.dataset):4d} samples \"\n",
    "              f\"· {len(te):3d} batches\")\n",
    "\n",
    "        # -------- 取一个 batch 测 shape --------\n",
    "        batch = next(iter(tl))\n",
    "        keys  = list(batch.keys())\n",
    "        print(\"  --> keys:\", keys)\n",
    "\n",
    "        mri, pet, tab, y = (batch[\"MRI\"], batch[\"PET\"],\n",
    "                            batch[\"table\"], batch[\"label\"])\n",
    "        print(f\"      MRI   : {tuple(mri.shape)}  {mri.dtype}\")\n",
    "        print(f\"      PET   : {tuple(pet.shape)}  {pet.dtype}\")\n",
    "        print(f\"      table : {tuple(tab.shape)} {tab.dtype}  \"\n",
    "              f\"(should be [B, 192])\")\n",
    "        print(f\"      label : {tuple(y.shape)}   {y.dtype}\")\n",
    "\n",
    "        # -------- 快速看标签分布 --------\n",
    "        uniq, cnt = torch.unique(y, return_counts=True)\n",
    "        dist = {int(k): int(v) for k, v in zip(uniq, cnt)}\n",
    "        print(\"      label counts:\", dist)\n",
    "\n",
    "# 调用\n",
    "inspect_fold_loaders(fold_loaders)"
   ],
   "id": "1f134b7df0745589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "  ▸ train_loader ─  335 samples ·  56 batches (batch=6)\n",
      "  ▸ val_loader   ─   48 samples ·   8 batches\n",
      "  ▸ test_loader  ─   96 samples ·  16 batches\n",
      "  --> keys: ['MRI', 'PET', 'label', 'Subject', 'table']\n",
      "      MRI   : (6, 1, 91, 109, 91)  torch.float32\n",
      "      PET   : (6, 1, 91, 109, 91)  torch.float32\n",
      "      table : (6, 192) torch.float32  (should be [B, 192])\n",
      "      label : (6,)   torch.int64\n",
      "      label counts: {0: 3, 1: 3}\n",
      "\n",
      "=== Fold 2 ===\n",
      "  ▸ train_loader ─  335 samples ·  56 batches (batch=6)\n",
      "  ▸ val_loader   ─   48 samples ·   8 batches\n",
      "  ▸ test_loader  ─   96 samples ·  16 batches\n",
      "  --> keys: ['MRI', 'PET', 'label', 'Subject', 'table']\n",
      "      MRI   : (6, 1, 91, 109, 91)  torch.float32\n",
      "      PET   : (6, 1, 91, 109, 91)  torch.float32\n",
      "      table : (6, 192) torch.float32  (should be [B, 192])\n",
      "      label : (6,)   torch.int64\n",
      "      label counts: {0: 5, 1: 1}\n",
      "\n",
      "=== Fold 3 ===\n",
      "  ▸ train_loader ─  335 samples ·  56 batches (batch=6)\n",
      "  ▸ val_loader   ─   48 samples ·   8 batches\n",
      "  ▸ test_loader  ─   96 samples ·  16 batches\n",
      "  --> keys: ['MRI', 'PET', 'label', 'Subject', 'table']\n",
      "      MRI   : (6, 1, 91, 109, 91)  torch.float32\n",
      "      PET   : (6, 1, 91, 109, 91)  torch.float32\n",
      "      table : (6, 192) torch.float32  (should be [B, 192])\n",
      "      label : (6,)   torch.int64\n",
      "      label counts: {0: 4, 1: 2}\n",
      "\n",
      "=== Fold 4 ===\n",
      "  ▸ train_loader ─  335 samples ·  56 batches (batch=6)\n",
      "  ▸ val_loader   ─   48 samples ·   8 batches\n",
      "  ▸ test_loader  ─   96 samples ·  16 batches\n",
      "  --> keys: ['MRI', 'PET', 'label', 'Subject', 'table']\n",
      "      MRI   : (6, 1, 91, 109, 91)  torch.float32\n",
      "      PET   : (6, 1, 91, 109, 91)  torch.float32\n",
      "      table : (6, 192) torch.float32  (should be [B, 192])\n",
      "      label : (6,)   torch.int64\n",
      "      label counts: {0: 4, 1: 2}\n",
      "\n",
      "=== Fold 5 ===\n",
      "  ▸ train_loader ─  336 samples ·  56 batches (batch=6)\n",
      "  ▸ val_loader   ─   48 samples ·   8 batches\n",
      "  ▸ test_loader  ─   95 samples ·  16 batches\n",
      "  --> keys: ['MRI', 'PET', 'label', 'Subject', 'table']\n",
      "      MRI   : (6, 1, 91, 109, 91)  torch.float32\n",
      "      PET   : (6, 1, 91, 109, 91)  torch.float32\n",
      "      table : (6, 192) torch.float32  (should be [B, 192])\n",
      "      label : (6,)   torch.int64\n",
      "      label counts: {0: 5, 1: 1}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:54:21.062394Z",
     "start_time": "2025-07-14T05:54:20.864812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------- 创建模型 -------------------\n",
    "from models.mmad_encoder import ImageEncoder, ImageEncoder_CEN  # 根据你的命名调整\n",
    "from models.mmad_encoder import MultiModalClassifier\n",
    "def generate_image_model(cfg):\n",
    "    # 使用 CEN 版本的编码器 + 分类头\n",
    "    model = ImageEncoder_CEN(\n",
    "        in_ch_modality   = 1,\n",
    "        level_channels   = [64, 128, 256],\n",
    "        bottleneck_ch    = 512,\n",
    "        share_layers     = 2,\n",
    "        cen_ratios       = (0.20, 0.10),\n",
    "    ).to(cfg.device)\n",
    "\n",
    "    # 参数统计\n",
    "    total_params     = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    bytes_per_param  = 2 if getattr(cfg, 'fp16', False) else 4\n",
    "\n",
    "    print(\"-------------------- model --------------------\")\n",
    "    print(f\"Total params(M)    : {total_params:,}\")\n",
    "    print(f\"Trainable params(M): {trainable_params:,}\")\n",
    "    print(f\"Approx. size       : {total_params * bytes_per_param / 1024**2:.2f} MB\")\n",
    "    print(\"Model type:\", type(model).__name__)\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_mm_classifier(cfg):\n",
    "    model = MultiModalClassifier(\n",
    "        img_dim=1024,\n",
    "        tab_dim=192,\n",
    "        num_classes=2\n",
    "    ).to(cfg.device)\n",
    "    # 参数统计\n",
    "    total_params     = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    bytes_per_param  = 2 if getattr(cfg, 'fp16', False) else 4\n",
    "\n",
    "    print(\"-------------------- model --------------------\")\n",
    "    print(f\"Total params(M)    : {total_params:,}\")\n",
    "    print(f\"Trainable params(M): {trainable_params:,}\")\n",
    "    print(f\"Approx. size       : {total_params * bytes_per_param / 1024**2:.2f} MB\")\n",
    "    print(\"Model type:\", type(model).__name__)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = generate_image_model(cfg)\n",
    "model = generate_mm_classifier(cfg)\n",
    "print(model)"
   ],
   "id": "bf62256aa672099f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- model --------------------\n",
      "Total params(M)    : 13,667,328\n",
      "Trainable params(M): 13,667,328\n",
      "Approx. size       : 26.07 MB\n",
      "Model type: ImageEncoder_CEN\n",
      "-------------------- model --------------------\n",
      "Total params(M)    : 312,066\n",
      "Trainable params(M): 312,066\n",
      "Approx. size       : 0.60 MB\n",
      "Model type: MultiModalClassifier\n",
      "MultiModalClassifier(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1216, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-14T05:54:21.310839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------- 五折训练主循环 -----------------\n",
    "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for fold_idx in range(cfg.n_splits):\n",
    "    fold = fold_idx + 1\n",
    "    print(f\"\\n=== Fold {fold}/{cfg.n_splits} ===\")\n",
    "\n",
    "    # ---- 模型：影像编码器 + 多模态分类器 ---- #\n",
    "    img_encoder = generate_image_model(cfg).to(cfg.device)\n",
    "    clf_model   = generate_mm_classifier(cfg).to(cfg.device)\n",
    "\n",
    "    params = list(img_encoder.parameters()) + list(clf_model.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params, lr=cfg.lr, weight_decay=getattr(cfg, 'weight_decay', 0)\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.num_epochs)\n",
    "    scaler    = GradScaler(enabled=getattr(cfg, 'fp16', False))\n",
    "\n",
    "    # ---- DataLoader ---- #\n",
    "    tr_loader = fold_loaders[fold_idx]['train_loader']\n",
    "    vl_loader = fold_loaders[fold_idx]['val_loader']\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ---- CSV ---- #\n",
    "    csv_path = os.path.join(cfg.checkpoint_dir, f\"metrics_fold{fold}.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"epoch\", \"train_Loss\",\"train_ACC\",\"train_PRE\",\"train_SEN\",\"train_SPE\",\"train_F1\",\"train_AUC\",\"train_MCC\",\n",
    "            \"val_Loss\",\"val_ACC\",\"val_PRE\",\"val_SEN\",\"val_SPE\",\"val_F1\",\"val_AUC\",\"val_MCC\",\n",
    "        ])\n",
    "\n",
    "    best_auc = -np.inf\n",
    "\n",
    "    # -------------- Epoch 循环 --------------\n",
    "    for epoch in range(1, cfg.num_epochs + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # -------- Train --------\n",
    "        img_encoder.train(); clf_model.train()\n",
    "        tr_loss_sum, tr_batches = 0.0, 0\n",
    "        yt, yp, ys = [], [], []\n",
    "\n",
    "        for batch in tr_loader:\n",
    "            mri   = batch['MRI'].to(cfg.device)\n",
    "            pet   = batch['PET'].to(cfg.device)\n",
    "            table = batch['table'].to(cfg.device).float()\n",
    "            y     = batch['label'].to(cfg.device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(device_type='cuda', enabled=getattr(cfg, 'fp16', False)):\n",
    "                img_feat = img_encoder(mri, pet)          # [B, img_dim]\n",
    "                out      = clf_model(img_feat, table)     # [B, num_cls]\n",
    "                loss     = criterion(out, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            tr_loss_sum += loss.item(); tr_batches += 1\n",
    "            prob = torch.softmax(out, dim=1)[:, 1].detach().cpu().numpy()\n",
    "            pred = out.argmax(1).detach().cpu().numpy()\n",
    "            yt.extend(y.cpu().numpy()); yp.extend(pred); ys.extend(prob)\n",
    "\n",
    "        tr_met  = calculate_metrics(yt, yp, ys)\n",
    "        tr_loss = tr_loss_sum / tr_batches\n",
    "\n",
    "        # -------- Validation --------\n",
    "        img_encoder.eval(); clf_model.eval()\n",
    "        vl_loss_sum, vl_batches = 0.0, 0\n",
    "        yt, yp, ys = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in vl_loader:\n",
    "                mri   = batch['MRI'].to(cfg.device)\n",
    "                pet   = batch['PET'].to(cfg.device)\n",
    "                table = batch['table'].to(cfg.device).float()\n",
    "                y     = batch['label'].to(cfg.device).long()\n",
    "\n",
    "                with autocast(device_type='cuda', enabled=getattr(cfg, 'fp16', False)):\n",
    "                    img_feat = img_encoder(mri, pet)\n",
    "                    out      = clf_model(img_feat, table)\n",
    "                    loss     = criterion(out, y)\n",
    "\n",
    "                vl_loss_sum += loss.item(); vl_batches += 1\n",
    "                prob = torch.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "                pred = out.argmax(1).cpu().numpy()\n",
    "                yt.extend(y.cpu().numpy()); yp.extend(pred); ys.extend(prob)\n",
    "\n",
    "        vl_met  = calculate_metrics(yt, yp, ys)\n",
    "        vl_loss = vl_loss_sum / vl_batches\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Fold {fold} | Epoch {epoch:03d} | Train Loss={tr_loss:.4f} | Val Loss={vl_loss:.4f} | \"\n",
    "              f\"Train ACC={tr_met['ACC']:.4f} | Val ACC={vl_met['ACC']:.4f} | Train AUC={tr_met['AUC']:.4f} | Val AUC={vl_met['AUC']:.4f} | \"\n",
    "              f\"time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        # -------- Save best model --------\n",
    "        if vl_met['AUC'] > best_auc:\n",
    "            best_auc = vl_met['AUC']\n",
    "            torch.save({\n",
    "                'img_encoder': img_encoder.state_dict(),\n",
    "                'clf_model'  : clf_model.state_dict()\n",
    "            }, os.path.join(cfg.checkpoint_dir, f\"best_model_fold{fold}.pth\"))\n",
    "            print(f\"✅ Fold {fold} saved best model (AUC={best_auc:.4f})\")\n",
    "\n",
    "        # -------- CSV log --------\n",
    "        with open(csv_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                epoch,\n",
    "                f\"{tr_loss:.4f}\", f\"{tr_met['ACC']:.4f}\", f\"{tr_met['PRE']:.4f}\",\n",
    "                f\"{tr_met['SEN']:.4f}\", f\"{tr_met['SPE']:.4f}\", f\"{tr_met['F1']:.4f}\", f\"{tr_met['AUC']:.4f}\", f\"{tr_met['MCC']:.4f}\",\n",
    "                f\"{vl_loss:.4f}\", f\"{vl_met['ACC']:.4f}\", f\"{vl_met['PRE']:.4f}\",\n",
    "                f\"{vl_met['SEN']:.4f}\", f\"{vl_met['SPE']:.4f}\", f\"{vl_met['F1']:.4f}\", f\"{vl_met['AUC']:.4f}\", f\"{vl_met['MCC']:.4f}\",\n",
    "            ])\n",
    "\n",
    "    print(f\"=== Fold {fold} 完成，Best AUC={best_auc:.4f} ===\")\n"
   ],
   "id": "9c33d130d251000d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "-------------------- model --------------------\n",
      "Total params(M)    : 13,667,328\n",
      "Trainable params(M): 13,667,328\n",
      "Approx. size       : 26.07 MB\n",
      "Model type: ImageEncoder_CEN\n",
      "-------------------- model --------------------\n",
      "Total params(M)    : 312,066\n",
      "Trainable params(M): 312,066\n",
      "Approx. size       : 0.60 MB\n",
      "Model type: MultiModalClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongzj\\AppData\\Local\\Temp\\ipykernel_17084\\146870631.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler(enabled=getattr(cfg, 'fp16', False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | Epoch 001 | Train Loss=0.6993 | Val Loss=0.7077 | Train ACC=0.4746 | Val ACC=0.3333 | Train AUC=0.4321 | Val AUC=0.2383 | time=221.2s\n",
      "✅ Fold 1 saved best model (AUC=0.2383)\n",
      "Fold 1 | Epoch 002 | Train Loss=0.6792 | Val Loss=0.6771 | Train ACC=0.5970 | Val ACC=0.6667 | Train AUC=0.5154 | Val AUC=0.4023 | time=223.5s\n",
      "✅ Fold 1 saved best model (AUC=0.4023)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# -------------------- 测试 -----------------------------\n",
   "id": "18d7ee81fb5ed447",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
